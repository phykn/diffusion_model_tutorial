{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL Divergence\n",
    "\n",
    "KL divergence, short for Kullback-Leibler divergence, is a measure of how different two probability distributions are from each other. It is commonly used in statistics, information theory, and machine learning to compare two probability distributions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition\n",
    "In general, KL divergence is defined as:\n",
    "\n",
    "$$\n",
    "D_{KL}(P\\|Q) = \\int_{-\\infty}^{\\infty} p(x) \\log\\frac{p(x)}{q(x)} dx\n",
    "$$\n",
    "\n",
    "where $p(x)$ and $q(x)$ are probability density functions of two distributions $P$ and $Q$, respectively. When $P$ and $Q$ are discrete probability distributions, the integral is replaced by a sum, and the formula becomes:\n",
    "\n",
    "$$\n",
    "D_{KL}(P\\|Q) = \\sum_{i=1}^{n} p(i) \\log\\frac{p(i)}{q(i)}\n",
    "$$\n",
    "\n",
    "where $p(i)$ and $q(i)$ are the probabilities of the $i$-th outcome in distributions $P$ and $Q$, respectively."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jensen's inequality\n",
    "Jensen's inequality can be used to prove several important properties of KL divergence. Jensen's inequality is a fundamental theorem in mathematics that states that for any convex function $f(x)$ and any random variable $X$, the expectation of the function $f$ of $X$ is greater than or equal to the function of the expectation of $X$. Mathematically, it can be written as:\n",
    "\n",
    "$$\n",
    "f(\\mathbb{E}[X]) \\leq \\mathbb{E}[f(X)]\n",
    "$$\n",
    "\n",
    "or equivalently,\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[f(X)] - f(\\mathbb{E}[X]) \\geq 0\n",
    "$$\n",
    "\n",
    "Intuitively, Jensen's inequality tells us that the expectation of a function of a random variable is at least as large as the function of the expectation of the random variable. The equality holds only when $f(x)$ is a linear function of $x$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties of KL Divergence\n",
    "1. **KL Divergence is always non-negative.**    \n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   D_{\\text{KL}}(P||Q) &= -\\int p(x) \\text{log} \\frac{q(x)}{p(x)} dx \\\\\n",
    "   &= E \\left [ -\\text{log} \\frac{q(x)}{p(x)} \\right ] \\\\\n",
    "   &\\geq -\\text{log} E \\left [ \\frac{q(x)}{p(x)} \\right ] \\quad \\because -\\text{log}(x) \\text{ is a convex function.}\\\\\n",
    "   &= -\\text{log} \\int p(x) \\frac{q(x)}{p(x)} dx \\\\\n",
    "   &= -\\text{log} \\int q(x) dx \\\\\n",
    "   &= -\\text{log}(1) \\\\\n",
    "   &= 0 \\\\  \n",
    "   \\\\\n",
    "   \\therefore D_{\\text{KL}}(P||Q) & \\geq 0\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "2. **The cross-entropy is always greater than or equal to entropy.**\n",
    "   $$\n",
    "   \\begin{aligned} \n",
    "   D_{\\text{KL}}(P||Q) &= \\int p(x) \\text{log} \\frac{p(x)}{q(x)} dx \\\\\n",
    "   &= \\int p(x) \\text{log } p(x) dx - \\int p(x) \\text{log } q(x) dx \\\\\n",
    "   &\\geq 0 \\\\\n",
    "   \\\\\n",
    "   \\therefore -\\int p(x) \\text{log } q(x) dx &\\geq -\\int p(x) \\text{log } p(x) dx\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "   where $-\\int p(x) \\text{log } q(x) dx$ and $-\\int p(x) \\text{log } p(x) dx$ are the cross-entropy and the entropy, respectively.\n",
    "\n",
    "3. **Two univariate normal distributions $P$ and $Q$ are simplified to**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   D_{\\text{KL}}(P||Q) = \\text{log }\\frac{\\sigma_{q}}{\\sigma_{p}} + \\frac{\\sigma_{p}^{2} + (\\mu_{p}-\\mu_{q})^{2}}{2 \\sigma_{q}^{2}} - \\frac{1}{2}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "   $\\bf{proof}$\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   D_{\\text{KL}}(P||Q) &= \\int p(x) \\text{log} \\frac{p(x)}{q(x)} dx \\\\\n",
    "   &= E_{p} \\left [ \\text{log} \\frac{p(x)}{q(x)} \\right ] \\\\\n",
    "   &= E_{p} [ \\text{log } p(x) - \\text{log } q(x) ] \\\\\n",
    "   &= E_{p} [ \\text{log } p(x) ] - E_{p} [ \\text{log } q(x) ] \\\\\n",
    "   E_{p} [ \\text{log } p(x) ] &= E_{p} \\left \\{ \\text{log } \\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} \\text{exp} \\left [ {-\\frac{(x-\\mu_{p})^{2}}{2\\sigma_{p}^{2}}} \\right ] \\right \\} \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} - \\frac{1}{2\\sigma_{p}^{2}} E_{p} \\left [ (x-\\mu_{p})^{2} \\right ] \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} - \\frac{1}{2\\sigma_{p}^{2}} \\cdot \\sigma_{p}^{2} \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} - \\frac{1}{2} \\\\\n",
    "   E_{p} [ \\text{log } q(x) ] &= E_{p} \\left \\{ \\text{log } \\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} \\text{exp} \\left [ {-\\frac{(x-\\mu_{q})^{2}}{2\\sigma_{q}^{2}}} \\right ] \\right \\} \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{1}{2\\sigma_{q}^{2}} E_{p} \\left [ (x-\\mu_{q})^{2} \\right ] \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{1}{2\\sigma_{q}^{2}} E_{p} \\left [ x^{2} - 2x\\mu_{q} + \\mu_{q}^{2} \\right ] \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{1}{2\\sigma_{q}^{2}} E_{p} \\left [ x^{2} - 2x\\mu_{q} + \\mu_{q}^{2} - 2x\\mu_{p} + 2x\\mu_{p} + \\mu_{p}^{2} - \\mu_{p}^{2} \\right ] \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{1}{2\\sigma_{q}^{2}} E_{p} \\left [ (x^{2} - 2x\\mu_{p} + \\mu_{p}^{2}) - 2x\\mu_{q} + 2x\\mu_{p} + \\mu_{q}^{2} - \\mu_{p}^{2} \\right ] \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{1}{2\\sigma_{q}^{2}} \\left \\{ E_{p} [ (x-\\mu_{p})^{2} ] - 2\\mu_{p}\\mu_{q} + 2\\mu_{p}^{2} + \\mu_{q}^{2} - \\mu_{p}^{2} \\right \\} \\\\\n",
    "   &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} - \\frac{\\sigma_{p}^{2} + (\\mu_{p}-\\mu_{q})^{2}}{2\\sigma_{q}^{2}}  \\\\\n",
    "   D_{\\text{KL}}(P||Q) &= \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{p}} - \\frac{1}{2} - \\text{log }\\frac{1}{\\sqrt{2\\pi}\\sigma_{q}} + \\frac{\\sigma_{p}^{2} + (\\mu_{p}-\\mu_{q})^{2}}{2\\sigma_{q}^{2}} \\\\\n",
    "   &= \\text{log }\\frac{\\sigma_{q}}{\\sigma_{p}} + \\frac{\\sigma_{p}^{2} + (\\mu_{p}-\\mu_{q})^{2}}{2\\sigma_{q}^{2}} - \\frac{1}{2}\n",
    "   \\end{aligned}\n",
    "   $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b5f243489bd9358788296533fc03025fea49f65e08ef6aa7a40b96c7113e3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
