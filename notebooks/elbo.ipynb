{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evidence lower bound (ELBO)\n",
    "\n",
    "Evidence Lower Bound(ELBO) is a quantity that is used in variational inference to estimate the log marginal likelihood or evidence of a model given some observed data. \n",
    "\n",
    "The ELBO is expressed as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{ELBO} \\coloneqq E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi} (z)} \\right ]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $p_{\\theta}(x, z)$ is the joint distribution of the data $x$ and latent variables $z$, and $q_{\\phi} (z)$ is an approximate posterior distribution over $z$. The ELBO can be used to optimize the parameters of the approximate posterior distribution so that **it approximates the true posterior distribution as closely as possible.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why do we use ELBO?\n",
    "\n",
    "We use the ELBO in variational inference as a proxy to the log marginal likelihood, which is often intractable to compute directly. The ELBO provides a lower bound on the log marginal likelihood, which allows us to perform optimization on a tractable objective function. By optimizing the ELBO, we are indirectly maximizing the log marginal likelihood. Maximizing the ELBO is equivalent to minimizing the KL divergence between the true posterior distribution and an approximate distribution, which allows us to find the optimal approximation of the true posterior."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properties\n",
    "1. **The evidence is always larger than or equal to the ELBO. We refer to this inequality as the ELBO inequality.**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{log }p_{\\theta}(x) &= \\text{log} \\int p_{\\theta} (x|z) p(z) dz \\\\\n",
    "   &= \\text{log}\\int p_{\\theta}(x, z) dz \\\\\n",
    "   &= \\text{log}\\int p_{\\theta}(x, z) \\frac{q_{\\phi}(z)}{q_{\\phi}(z)} dz \\\\\n",
    "   &= \\text{log}\\int q_{\\phi}(z) \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} dz \\\\\n",
    "   &= \\text{log} E_{z \\sim q_{\\phi}} \\left [ \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\\\\n",
    "   &\\geq E_{z \\sim q_{\\phi}} \\left [\\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\quad \\because \\text{log}(x) \\text{ is a concave function.} \\\\\n",
    "   \\\\\n",
    "   \\therefore \\text{evidence} &\\geq \\text{ELBO}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "2. **KL Divergence between $p_{\\theta}(z|x)$ and $q_{\\phi}(z)$ equals $\\text{evidence} - \\text{ELBO}$.**\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   D_{\\text{KL}}(q_{\\phi}(z)||p_{\\theta}(z|x)) &= \\int q_{\\phi}(z) \\text{log} \\frac{q_{\\phi}(z)}{p_{\\theta}(z|x)} dz \\\\\n",
    "   &= E_{z \\sim q_{\\phi}} \\left [\\text{log}\\frac{q_{\\phi}(z)}{p_{\\theta}(z|x)} \\right ] \\\\\n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}}[\\text{log } p_{\\theta}(z|x)] \\\\\n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\left ( p_{\\theta}(z|x) \\frac{p_{\\theta}(x)}{p_{\\theta}(x)} \\right ) \\right ] \\\\ \n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(z, x)}{p_{\\theta}(x)} \\right ] \\\\ \n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} [ \\text{log } p_{\\theta}(z, x) ] + E_{z \\sim q_{\\phi}}[\\text{log } p_{\\theta}(x)] \\\\ \n",
    "   &= \\text{log } p_{\\theta}(x) - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\\\\n",
    "   &= \\text{evidence} - \\text{ELBO}\n",
    "   \\end{aligned}\n",
    "   $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Appendix: What is the log marginal likelihood or evidence?\n",
    "\n",
    "The log marginal likelihood (also known as evidence) measures the probability of observing the given data, averaged over all possible values of the model parameters. It is defined as:\n",
    "\n",
    "$$\\log p(x) = \\log \\int p(x|\\theta)p(\\theta) d\\theta$$\n",
    "\n",
    "where $x$ is the observed data, $\\theta$ are the model parameters, $p(x|\\theta)$ is the likelihood function, and $p(\\theta)$ is the prior distribution of the parameters.\n",
    "\n",
    "The log marginal likelihood is often used as a model selection criterion, since it penalizes complex models that overfit the data by assigning low probability to them. In Bayesian inference, the log marginal likelihood is also used to compute the Bayes factor, which compares the evidence of two competing models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b5f243489bd9358788296533fc03025fea49f65e08ef6aa7a40b96c7113e3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
