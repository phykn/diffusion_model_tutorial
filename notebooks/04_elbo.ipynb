{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence lower bound (ELBO)\n",
    "In variational Bayesian methods, the evidence lower bound (often abbreviated ELBO) is a useful lower bound on the log-likelihood of some observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{ELBO} \\coloneqq E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi} (z)} \\right ]\n",
    "\\end{aligned}\n",
    "$$\n",
    "where $p_{\\theta}(x, z)$ is joint distribution of $x$ and $z$. $\\theta$ and $\\phi$ are parameters.\n",
    "\n",
    "ELBO is used to obtain the lower bound of the evidence (or log evidence). The evidence is the likelihood function evaluated at a fixed $\\theta$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{evidence} \\coloneqq \\text{log }p_{\\theta}(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties\n",
    "1. The evidence is always larger than ELBO. We refer to the these inequality as the ELBO inequality.\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   \\text{log }p_{\\theta}(x) &= \\text{log} \\int p_{\\theta} (x|z) p(z) dz \\\\\n",
    "   &= \\text{log}\\int p_{\\theta}(x, z) dz \\\\\n",
    "   &= \\text{log}\\int p_{\\theta}(x, z) \\frac{q_{\\phi}(z)}{q_{\\phi}(z)} dz \\\\\n",
    "   &= \\text{log}\\int q_{\\phi}(z) \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} dz \\\\\n",
    "   &= \\text{log} E_{z \\sim q_{\\phi}} \\left [ \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\\\\n",
    "   &\\geq E_{z \\sim q_{\\phi}} \\left [\\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\quad \\because \\text{log}(x) \\text{ is a concave function.} \\\\\n",
    "   \\\\\n",
    "   \\therefore \\text{evidence} &\\geq \\text{ELBO}\n",
    "   \\end{aligned}\n",
    "   $$\n",
    "\n",
    "2. KL Divergence between $p_{\\theta}(z|x)$ and $q_{\\phi}(z)$ equals $\\text{evidence} - \\text{ELBO}$.\n",
    "   $$\n",
    "   \\begin{aligned}\n",
    "   D_{\\text{KL}}(q_{\\phi}(z)||p_{\\theta}(z|x)) &= \\int q_{\\phi}(z) \\text{log} \\frac{q_{\\phi}(z)}{p_{\\theta}(z|x)} dz \\\\\n",
    "   &= E_{z \\sim q_{\\phi}} \\left [ \\frac{q_{\\phi}(z)}{p_{\\theta}(z|x)} \\right ] \\\\\n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}}[\\text{log } p_{\\theta}(z|x)] \\\\\n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log }q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\left ( p_{\\theta}(z|x) \\frac{p_{\\theta}(x)}{p_{\\theta}(x)} \\right ) \\right ] \\\\ \n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log}q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(z, x)}{p_{\\theta}(x)} \\right ] \\\\ \n",
    "   &= E_{z \\sim q_{\\phi}}[\\text{log}q_{\\phi}(z)] - E_{z \\sim q_{\\phi}} [ \\text{log } p_{\\theta}(z, x) ] + E_{z \\sim q_{\\phi}}[\\text{log } p_{\\theta}(x)] \\\\ \n",
    "   &= \\text{log } p_{\\theta}(x) - E_{z \\sim q_{\\phi}} \\left [ \\text{log} \\frac{p_{\\theta}(x, z)}{q_{\\phi}(z)} \\right ] \\\\\n",
    "   &= \\text{evidence} - \\text{ELBO}\n",
    "   \\end{aligned}\n",
    "   $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "65b5f243489bd9358788296533fc03025fea49f65e08ef6aa7a40b96c7113e3c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
